"""
confusion.py - G√©n√©ration de la Matrice de Confusion

Ce script g√©n√®re une matrice de confusion pour √©valuer les performances du mod√®le V2.
La matrice de confusion est un tableau 20x20 qui montre pour chaque classe r√©elle (lignes)
combien de fois elle a √©t√© pr√©dite dans chaque classe (colonnes).

Utilit√©:
- Diagonale = pr√©dictions correctes (plus la diagonale est fonc√©e, mieux c'est)
- Hors diagonale = confusions entre classes (r√©v√®le quels avions sont confondus)
- Permet d'identifier les faiblesses du mod√®le (ex: A15 confondu avec A13)

La matrice est normalis√©e en pourcentages pour faciliter la lecture.
"""

import torch  # PyTorch pour charger le mod√®le et faire l'inf√©rence
import numpy as np  # NumPy pour calculs matriciels (normalisation, etc.)
import matplotlib.pyplot as plt  # Matplotlib pour cr√©er le graphique
import seaborn as sns  # Seaborn pour am√©liorer le rendu visuel de la heatmap
from sklearn.metrics import confusion_matrix  # Fonction de scikit-learn pour calculer la matrice
import os  # Manipulation des chemins
from tqdm import tqdm  # Barre de progression pour suivre l'avancement
import torchvision.transforms as T  # Transformations d'images

# Import des fichiers locaux du projet
import config  # Configuration centralis√©e (chemins, device, classes)
from model import get_model_instance_segmentation  # Architecture Faster R-CNN
from dataset import PlaneDataset  # Dataset personnalis√©

def run_full_confusion_matrix():
    """
    Fonction principale qui orchestre la g√©n√©ration de la matrice de confusion.
    
    √âtapes:
    1. Charge le mod√®le V2 entra√Æn√©
    2. Effectue l'inf√©rence sur toutes les images du dataset
    3. Compare les pr√©dictions aux annotations r√©elles (ground truth)
    4. Calcule et normalise la matrice de confusion
    5. G√©n√®re et sauvegarde le graphique
    """
    
    # ========== AFFICHAGE DES INFORMATIONS SYST√àME ==========
    print(f"‚öôÔ∏è Racine du projet : {config.ROOT_DIR}")
    print(f"üñ•Ô∏è Calcul de la matrice sur : {config.DEVICE}")

    # ========== 1. CONSTRUCTION DES CHEMINS ==========
    # Chemin vers le mod√®le V2 entra√Æn√© (dans le dossier models/)
    model_path = os.path.join(config.MODELS_DIR, 'faster_rcnn_avions_V2.pth')
    
    # Chemin de sortie pour sauvegarder l'image de la matrice (dans outputs/)
    output_path = os.path.join(config.OUTPUTS_DIR, 'matrice_confusion_TOTALE.png')

    # ========== 2. CHARGEMENT DU MOD√àLE V2 ==========
    # Cr√©ation de l'architecture (21 classes)
    model = get_model_instance_segmentation(config.NUM_CLASSES)
    
    # V√©rification de l'existence du fichier .pth
    if not os.path.exists(model_path):
        print(f"‚ùå Erreur : Mod√®le introuvable √† {model_path}")
        return  # Sort de la fonction si le mod√®le n'existe pas

    # Chargement des poids V2 dans le mod√®le
    # map_location=config.DEVICE assure la compatibilit√© GPU/CPU
    model.load_state_dict(torch.load(model_path, map_location=config.DEVICE))
    
    # D√©placement du mod√®le sur GPU/CPU et activation du mode √©valuation
    model.to(config.DEVICE)
    model.eval()  # Mode eval: d√©sactive dropout, fige batch norm (crucial pour inf√©rence)

    # ========== 3. PR√âPARATION DU DATASET COMPLET ==========
    # Chargement de TOUTES les images du dataset (environ 1331 images)
    # ToTensor() convertit les images PIL en tensors PyTorch
    dataset = PlaneDataset(config.DATA_DIR, transforms=T.Compose([T.ToTensor()]))
    
    # DataLoader pour charger les images une par une
    data_loader = torch.utils.data.DataLoader(
        dataset,
        batch_size=1,  # Une image √† la fois (simplifie la comparaison pr√©diction vs v√©rit√©)
        shuffle=False,  # Pas de m√©lange (pas n√©cessaire pour l'√©valuation)
        collate_fn=lambda x: tuple(zip(*x))  # Assemblage des batchs
    )

    # ========== 4. INITIALISATION DES LISTES DE COLLECTE ==========
    # Liste qui stockera toutes les pr√©dictions du mod√®le
    all_preds = []
    
    # Liste qui stockera toutes les vraies classes (ground truth des annotations XML)
    all_gt = []

    print(f"üîé Analyse compl√®te de {len(dataset)} images en cours...")
    
    # ========== 5. BOUCLE D'INF√âRENCE ==========
    # torch.no_grad() d√©sactive le calcul des gradients (√©conomise m√©moire et acc√©l√®re)
    with torch.no_grad():
        
        # It√©ration sur toutes les images avec barre de progression
        for images, targets in tqdm(data_loader):
            
            # D√©placement des images sur GPU/CPU
            # List comprehension qui applique .to(DEVICE) √† chaque image
            images = [img.to(config.DEVICE) for img in images]
            
            # Inf√©rence: le mod√®le retourne les pr√©dictions
            # outputs est une liste de dicts [{boxes, labels, scores}, ...]
            outputs = model(images)
            
            # ========== TRAITEMENT DE CHAQUE IMAGE DU BATCH ==========
            # (ici batch_size=1 donc une seule it√©ration)
            for i in range(len(targets)):
                
                # Extraction des labels r√©els (ground truth) depuis les annotations
                # .cpu() d√©place le tensor du GPU vers le CPU
                # .numpy() convertit le tensor PyTorch en array NumPy
                gt_labels = targets[i]['labels'].cpu().numpy()
                
                # Extraction des labels pr√©dits par le mod√®le
                pred_labels = outputs[i]['labels'].cpu().numpy()
                
                # Extraction des scores de confiance des pr√©dictions
                pred_scores = outputs[i]['scores'].cpu().numpy()
                
                # ========== FILTRAGE DES PR√âDICTIONS PAR SEUIL DE CONFIANCE ==========
                # Seuil √† 0.4 (40%) pour capter m√™me les d√©tections incertaines
                # Plus bas que 0.5 pour ne pas rater les vraies d√©tections avec confiance moyenne
                mask = pred_scores > 0.4
                
                # Indexation bool√©enne: garde seulement les labels o√π mask=True
                # valid_preds contient les labels des d√©tections > 40% de confiance
                valid_preds = pred_labels[mask]
                
                # ========== AJOUT √Ä LA LISTE SI D√âTECTION VALIDE ==========
                # V√©rifie qu'il y a au moins une pr√©diction valide ET au moins un objet r√©el
                if len(valid_preds) > 0 and len(gt_labels) > 0:
                    # SIMPLIFICATION: On compare seulement le 1er avion pr√©dit au 1er avion r√©el
                    # Une vraie matrice devrait matcher les bo√Ætes par IoU (Intersection over Union)
                    # mais cette simplification suffit pour avoir une vue d'ensemble
                    all_preds.append(valid_preds[0])  # Premier avion pr√©dit
                    all_gt.append(gt_labels[0])       # Premier avion r√©el

    # ========== 6. CALCUL DE LA MATRICE DE CONFUSION ==========
    # labels_range : Liste des classes [1, 2, ..., 20] (on ignore Background=0)
    labels_range = list(range(1, 21))
    
    # confusion_matrix() de scikit-learn calcule la matrice
    # cm[i, j] = nombre de fois o√π la vraie classe i a √©t√© pr√©dite comme j
    # Retourne une matrice NumPy 20x20
    cm = confusion_matrix(all_gt, all_preds, labels=labels_range)

    # ========== 7. NORMALISATION DE LA MATRICE (CONVERSION EN POURCENTAGES) ==========
    # Contexte pour ignorer les warnings de division par z√©ro (si une classe n'a aucun exemple)
    with np.errstate(divide='ignore', invalid='ignore'):
        
        # Conversion en float pour permettre la division
        # cm.sum(axis=1) : Somme par ligne (total de chaque classe r√©elle)
        # [:, np.newaxis] : Transforme array 1D en colonne 2D pour broadcasting
        # Division : chaque ligne est divis√©e par son total
        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        # np.nan_to_num() remplace les NaN (0/0) par 0
        # Se produit si une classe n'appara√Æt jamais dans le dataset
        cm_norm = np.nan_to_num(cm_norm)

    # ========== 8. CR√âATION DU GRAPHIQUE ==========
    # Cr√©ation d'une figure de grande taille (18x14 pouces) pour accueillir 20x20 cases
    plt.figure(figsize=(18, 14))
    
    # sns.heatmap() cr√©e une carte de chaleur (heatmap) color√©e
    sns.heatmap(
        cm_norm,           # Donn√©es: matrice normalis√©e
        annot=True,        # Affiche les valeurs num√©riques dans chaque case
        fmt='.2f',         # Format √† 2 d√©cimales (ex: 0.98)
        cmap='Greens',     # Palette de couleurs (blanc ‚Üí vert fonc√©)
        xticklabels=config.CLASSES[1:],  # Labels des colonnes (A1 √† A20)
        yticklabels=config.CLASSES[1:]   # Labels des lignes (A1 √† A20)
    )
    # config.CLASSES[1:] : Slice qui prend tout sauf Background (index 0)
    
    # ========== 9. AJOUT DES LABELS ET TITRE ==========
    plt.title(f'Matrice de Confusion Globale ({len(dataset)} images) - Mod√®le Elite', fontsize=18)
    plt.xlabel('Pr√©dictions de l\'IA', fontsize=14)  # Axe X = ce que le mod√®le pr√©dit
    plt.ylabel('V√©rit√© Terrain (Annotations XML)', fontsize=14)  # Axe Y = vraie classe
    
    # ========== 10. SAUVEGARDE ET AFFICHAGE ==========
    # Sauvegarde de la figure en PNG dans le dossier outputs/
    plt.savefig(output_path)
    
    # Affichage de la figure √† l'√©cran (fen√™tre interactive)
    plt.show()
    
    print(f"‚úÖ Matrice finale sauvegard√©e dans : {output_path}")

# ========== POINT D'ENTR√âE DU SCRIPT ==========
# Ex√©cute run_full_confusion_matrix() si le script est lanc√© directement
# (pas si import√© avec 'import confusion')
if __name__ == "__main__":
    run_full_confusion_matrix()
